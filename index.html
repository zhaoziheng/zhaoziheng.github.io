<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Ziheng Zhao (赵子恒)</title>

    <meta name="author" content="Ziheng Zhao">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/robot.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:0%;width:60%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  <name> Ziheng Zhao (赵子恒) </name>
                </p>
                <p>I'm a PhD candidate at <a href="https://www.sjtu.edu.cn/">Shanghai Jiao Tong University (SJTU)</a> , 
                  advised by <a href="https://weidixie.github.io/index.html">Prof. Weidi Xie</a>. 
                </p>
                <p>
                  My current research interests is in Artificial Intelligence for Medical (AI4Med).
                </p>
                <p>
                  E-mail: Zhao_Ziheng@sjtu.edu.cn <br>
                </p>

                <p style="text-align:center">
                  <a href="mailto:Zhao_Ziheng@sjtu.edu.cn">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?view_op=list_works&hl=zh-CN&user=NYNKEhYAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/zhaoziheng">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="figures/ziheng.jpg">
                  <img style="width:70%;max-width:70%;object-fit: cover; " alt="profile photo" src="figures/ziheng.jpg" class="hoverZoomLink">
                </a>
                </td>
              </tr>
              </tbody></table>

          <br>
          <br>
          <hr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:-150px;"><tbody>
            
            <tr>
              <td style="padding:0px" colspan="2">
                <p style="text-align:right;font-size:small;margin-bottom:30px;">
                  <sup>*</sup> denotes equal contribution.
                </p>
              </td>
            </tr>
            
            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;width:25%;vertical-align:middle">
                <img src="figures/OminiAbnormCT.png" width="250" style="border-style: none">
              </td>
              <td style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2506.03238">
                  <papertitle>Rethinking Whole-Body CT Image Interpretation: An Abnormality-Centric Approach</papertitle>
                </a>
                <br>
                <strong>Ziheng Zhao<sup>*</sup></strong>,
                <a>Lisong Dai<sup>*</sup></a>,
                <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
                <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang</a>,
                <a href="https://weidixie.github.io/">Weidi Xie</a>,
                <br>
                <em>Under Review, 2025.</em>
                <p>
                  In this work, we advanced abnormality-centric CT interpretation with: (1) A taxonomy of 404 abnormalities developed with radiologists; (2) OminiAbnorm-CT-14K, a dataset of 14.5K CT images, covering different views and whole-body regions, and 19K grounded abnormalities, each linked to a detailed description and cast into the taxonomy ; (3) The OminiAbnorm-CT framework for grounded CT interpretation, allowing flexible text guidance and visual prompts.
                </p>
              </td>
            </tr>
          <br>
            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;width:25%;vertical-align:middle">
                <img src="figures/RadIR.png" width="250" style="border-style: none">
              </td>
              <td style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2503.04653">
                  <papertitle>RadIR: A Scalable Framework for Multi-Grained Medical Image Retrieval via Radiology Report Mining</papertitle>
                </a>
                <br>
                <a>Tengfei Zhang<sup>*</sup></a>,
                <strong>Ziheng Zhao<sup>*</sup></strong>,
                <a href="https://chaoyi-wu.github.io">Chaoyi Wu</a>,
                <a>Xiao Zhou</a>,
                <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
                <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang</a>,
                <a href="https://weidixie.github.io/">Weidi Xie</a>,
                <br>
                <em>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), 2025.</em>
                <p>
                 In this paper, we proposed a scalable framework for multi-grained radiology image retrieval conditioned on text prompts.
                </p>
              </td>
            </tr>
          <br>
            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;width:25%;vertical-align:middle">
                <img src="figures/MRGen.png" width="250" style="border-style: none">
              </td>
              <td style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2412.04106">
                  <papertitle>MRGen: Diffusion-based Controllable Data Engine for MRI Segmentation towards Unannotated Modalities</papertitle>
                </a>
                <br>
                <a href="https://haoningwu3639.github.io/">Haoning Wu<sup>*</sup></a>,
                <strong>Ziheng Zhao<sup>*</sup></strong>,
                <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
                <a href="https://weidixie.github.io/">Weidi Xie</a>,
                <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang</a>,
                <br>
                <em>International Conference on Computer Vision (ICCV), 2025.</em>
                <p>
                In this paper, we propose MRGen, which can controllably synthesize data for unannotated modalities, without requiring registered data pairs, and extend segmentation models to unannotated modalities.
                </p>
              </td>
            </tr>  
          <br>
            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;width:25%;vertical-align:middle">
                <img src="figures/LoRKD.png" width="250" style="border-style: none">
              </td>
              <td style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2409.19540">
                  <papertitle>LoRKD: Low-Rank Knowledge Decomposition for Medical Foundation Models</papertitle>
                </a>
                <br>
                <a>HaoLin Li</a>,
                <a>Yuhang Zhou</a>,
                <strong>Ziheng Zhao</strong>,
                <a>Siyuan Du</a>,
                <a href="https://sunarker.github.io/">Jiangchao Yao</a>,
                <a href="https://weidixie.github.io/">Weidi Xie</a>,
                <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
                <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang</a>,
                <br>
                <em>Under Review, 2024.</em>
                <p>
                In this paper, we propose a novel framework named Low-Rank Knowledge Decomposition (LoRKD), to break down the foundation model into multiple lightweight expert models, each tailored to a specific domain. The goal of this paradigm is to improve the specialization of deployment models within a specific domain, while simultaneously reducing deployment costs.
                </p>
              </td>
            </tr>  
          <br>
            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;width:25%;vertical-align:middle">
                <img src="figures/SAT.png" width="250" style="border-style: none">
              </td>
              <td style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2312.17183">
                  <papertitle>Large-Vocabulary Segmentation for Medical Images with Text Prompts</papertitle>
                </a>
                <br>
                <strong>Ziheng Zhao</strong>,
                <a href="https://github.com/YaoZhang93">Yao Zhang</a>,
                <a href="https://chaoyi-wu.github.io">Chaoyi Wu</a>,
                <a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang</a>,
                <a>Xiao Zhou</a>,
                <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
                <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang</a>,
                <a href="https://weidixie.github.io/">Weidi Xie</a>,
                <br>
                <em>npj Digital Medicine, 2025.</em>
                <p>
                 In this paper, we build up a large-vocabulary segmentation model for medical image, driven by text prompts (SAT).
                </p>
              </td>
            </tr>
          <br>
            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;width:25%;vertical-align:middle">
                <img src="figures/RadGenome_ChestCT.png" width="250" style="border-style: none">
              </td>
              <td style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2402.13963">
                  <papertitle>RadGenome-Chest CT: A Grounded Vision-Language Dataset for Chest CT Analysis</papertitle>
                </a>
                <br>
                <a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang</a>,
                <a href="https://chaoyi-wu.github.io">Chaoyi Wu</a>,
                <strong>Ziheng Zhao</strong>,
                <a>Jiayu Lei</a>,
                <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
                <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang</a>,
                <a href="https://weidixie.github.io/">Weidi Xie</a>,
                <br>
                <em>Under Review, 2024.</em>
                <p>
                In this paper, we introduce RadGenome-Chest CT, a comprehensive, large-scale, region-guided 3D chest CT interpretation dataset based on CT-RATE. It includes: Organ-level segmentation for 197 categories; 665K multi-granularity grounded reports; 1.3M grounded VQA pairs.
                </p>
              </td>
            </tr>  
          <br>
            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;width:25%;vertical-align:middle">
                <img src="figures/GPT_4V_Eval.png" width="250" style="border-style: none">
              </td>
              <td style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2310.09909">
                  <papertitle>Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis</papertitle>
                </a>
                <br>
                <a href="https://chaoyi-wu.github.io">Chaoyi Wu<sup>*</sup></a>,
                <a>Jiayu Lei<sup>*</sup></a>,
                <a href="https://qiaoyu-zheng.github.io">Qiaoyu Zheng<sup>*</sup></a>,
                <a href="https://angelakeke.github.io">Weike Zhao<sup>*</sup></a>,
                <a href="https://weixionglin.github.io/me/">Weixiong Lin<sup>*</sup></a>,
                <a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang<sup>*</sup></a>,
                <a>Xiao Zhou<sup>*</sup></a>,
                <strong>Ziheng Zhao<sup>*</sup></strong>,
                <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang</a>,
                <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
                <a href="https://weidixie.github.io/">Weidi Xie</a>,
                <br>
                <em>Technical Report, 2023.</em>
                <p>
                 We evaluate the GPT-4V on 92 radiographic cases, 20 pathoglogy cases and 16 location cases across 17 medical systems covering 8 imaging modalities. In general, as the cases shown, GPT-4V is still  far from clinical usage.
                </p>
              </td>
            </tr> 
          <br>
            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;width:25%;vertical-align:middle">
                <img src="figures/PMC-VQA.png" width="250" style="border-style: none">
              </td>
              <td style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2305.10415.pdf">
                  <papertitle>PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering</papertitle>
                </a>
                <br>
                <a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang<sup>*</sup></a>,
                <a href="https://chaoyi-wu.github.io">Chaoyi Wu<sup>*</sup></a>,
                <strong>Ziheng Zhao</strong>,
                <a href="https://weixionglin.github.io/me/">Weixiong Lin</a>,
                <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang </a>,
                <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
                <a href="https://weidixie.github.io/">Weidi Xie</a>,
                <br>
                <em>Communications Medicine, 2024.</em>
                <p>
                  In this paper, we focus on the problem of Medical Visual Question Answering (MedVQA). We propose a generative medical VQA model, MedVInT, together with a large scale MedVQA Dataset, PMC-VQA.
                </p>
              </td>
            </tr>
          <br>
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="figures/PMC_CLIP.png" width="250" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2303.07240">
                <papertitle>PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents</papertitle>
              </a>
              <br>
              <a href="https://weixionglin.github.io/me/">Weixiong Lin<sup>*</sup></a>,
              <strong>Ziheng Zhao<sup>*</sup></strong>,
              <a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang</a>,
              <a href="https://chaoyi-wu.github.io">Chaoyi Wu</a>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang </a>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              <a href="https://weidixie.github.io/">Weidi Xie</a>,
              <br>
              <em>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), 2023.</em>
              <p>
              We collect a biomedical dataset, PMC-OA with <strong>1.6M</strong> image-caption pairs collected from PubMedCentral's OpenAccess subset.
              </p>
            </td>
            </tr>
            <br>
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="figures/K-Space-Transformer.jpg" width="250" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2206.06947">
                <papertitle>K-Space Transformer for Undersampled MRI Reconstruction</papertitle>
              </a>
              <br>
              <strong>Ziheng Zhao</strong>,
              <a>Tianjiao Zhang</a>,
              <a href="https://weidixie.github.io/">Weidi Xie</a>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang </a>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              <br>
              <em>British Machine Vision Conference (BMVC), 2022.</em>
              <p>
                We propose a novel Transformer-based framework to reconstruct undersampled MRI directly in k-space.
              </p>
            </td>
            </tr>

          </td>
        </tr>
      </table>
  </body>
</html>
