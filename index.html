<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Ziheng Zhao (赵子恒)</title>

    <meta name="author" content="Ziheng Zhao">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/robot.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:0%;width:60%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  <name> Ziheng Zhao (赵子恒) </name>
                </p>
                <p>I'm a PhD candidate at <a href="https://www.sjtu.edu.cn/">Shanghai Jiao Tong University (SJTU)</a> , 
                  advised by <a href="https://weidixie.github.io/index.html">Prof. Weidi Xie</a>. 
                </p>
                <p>
                  My current research interests is in Artificial Intelligence for Medical (AI4Med).
                </p>
                <p>
                  E-mail: Zhao_Ziheng@sjtu.edu.cn <br>
                </p>

                <p style="text-align:center">
                  <a href="Zhao_Ziheng@sjtu.edu.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?view_op=list_works&hl=zh-CN&user=NYNKEhYAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/zhaoziheng">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="figures/ziheng.jpg">
                  <img style="width:70%;max-width:70%;object-fit: cover; " alt="profile photo" src="figures/ziheng.jpg" class="hoverZoomLink">
                </a>
                </td>
              </tr>
              </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:0px;"><tbody>
            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;width:25%;vertical-align:middle">
                <img src="figures/SAT.png" width="250" style="border-style: none">
              </td>
              <td style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2312.17183">
                  <papertitle>One Model to Rule them All: Towards Universal Segmentation for Medical Images with Text Prompts</papertitle>
                </a>
                <br>
                <strong>Ziheng Zhao</strong>,
                <a href="https://github.com/YaoZhang93">Yao Zhang</a>,
                <a href="https://chaoyi-wu.github.io">Chaoyi Wu</a>,
                <a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang</a>,
                <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
                <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang</a>,
                <a href="https://weidixie.github.io/">Weidi Xie</a>,
                <br>
                <em>In submission, 2024.</em>
                <p>
                 In this paper, we build up a universal medical segmentation model, driven by text prompts (SAT).
                </p>
              </td>
            </tr>
          <br>
            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;width:25%;vertical-align:middle">
                <img src="figures/RadGenome_ChestCT.png" width="250" style="border-style: none">
              </td>
              <td style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2402.13963">
                  <papertitle>RadGenome-Chest CT: A Grounded Vision-Language Dataset for Chest CT Analysis</papertitle>
                </a>
                <br>
                <a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang</a>,
                <a href="https://chaoyi-wu.github.io">Chaoyi Wu</a>,
                <strong>Ziheng Zhao</strong>,
                <a>Jiayu Lei</a>,
                <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
                <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang</a>,
                <a href="https://weidixie.github.io/">Weidi Xie</a>,
                <br>
                <em>In submission, 2024.</em>
                <p>
                In this paper, we introduce RadGenome-Chest CT, a comprehensive, large-scale, region-guided 3D chest CT interpretation dataset based on CT-RATE. It includes: Organ-level segmentation for 197 categories; 665K multi-granularity grounded reports; 1.3M grounded VQA pairs.
                </p>
              </td>
            </tr>  
          <br>
            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;width:25%;vertical-align:middle">
                <img src="figures/GPT_4V_Eval.png" width="250" style="border-style: none">
              </td>
              <td style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2310.09909">
                  <papertitle>Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis</papertitle>
                </a>
                <br>
                <a href="https://chaoyi-wu.github.io">Chaoyi Wu<sup>*</sup></a>,
                <a>Jiayu Lei<sup>*</sup></a>,
                <a href="https://qiaoyu-zheng.github.io">Qiaoyu Zheng<sup>*</sup></a>,
                <a href="https://angelakeke.github.io">Weike Zhao<sup>*</sup></a>,
                <a href="https://weixionglin.github.io/me/">Weixiong Lin<sup>*</sup></a>,
                <a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang<sup>*</sup></a>,
                <a>Xiao Zhou<sup>*</sup></a>,
                <strong>Ziheng Zhao<sup>*</sup></strong>,
                <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang</a>,
                <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
                <a href="https://weidixie.github.io/">Weidi Xie</a>,
                <br>
                <em>Technical Report, 2023.</em>
                <p>
                 We evaluate the GPT-4V on 92 radiographic cases, 20 pathoglogy cases and 16 location cases across 17 medical systems covering 8 imaging modalities. In general, as the cases shown, GPT-4V is still  far from clinical usage.
                </p>
              </td>
            </tr> 
          <br>
            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;width:25%;vertical-align:middle">
                <img src="figures/PMC-VQA.png" width="250" style="border-style: none">
              </td>
              <td style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2305.10415.pdf">
                  <papertitle>PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering</papertitle>
                </a>
                <br>
                <a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang<sup>*</sup></a>,
                <a href="https://chaoyi-wu.github.io">Chaoyi Wu<sup>*</sup></a>,
                <strong>Ziheng Zhao</strong>,
                <a href="https://weixionglin.github.io/me/">Weixiong Lin</a>,
                <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang </a>,
                <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
                <a href="https://weidixie.github.io/">Weidi Xie</a>,
                <br>
                <em>In submission, 2023.</em>
                <p>
                  In this paper, we focus on the problem of Medical Visual Question Answering (MedVQA). We propose a generative medical VQA model, MedVInT, together with a large scale MedVQA Dataset, PMC-VQA.
                </p>
              </td>
            </tr>
          <br>
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="figures/PMC_CLIP.png" width="250" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2303.07240">
                <papertitle>PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents</papertitle>
              </a>
              <br>
              <a href="https://weixionglin.github.io/me/">Weixiong Lin<sup>*</sup></a>,
              <strong>Ziheng Zhao<sup>*</sup></strong>,
              <a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang</a>,
              <a href="https://chaoyi-wu.github.io">Chaoyi Wu</a>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang </a>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              <a href="https://weidixie.github.io/">Weidi Xie</a>,
              <br>
              <em>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), 2023.</em>
              <p>
              We collect a biomedical dataset, PMC-OA with <strong>1.6M</strong> image-caption pairs collected from PubMedCentral's OpenAccess subset.
              </p>
            </td>
            </tr>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    <sup>*</sup> denotes equal contribution.
                    <br>
                  </p>
                </td>
              </tr>
            </tbody></table>
          </td>
        </tr>
      </table>
  </body>
</html>
